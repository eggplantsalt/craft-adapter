# CRaFT Implementation Progress Tracker

## Project Overview
å®ç° CRaFT (Constrained Representation and Fine-Tuning) ç®—æ³•åˆ° VLA-Adapter ä»£ç åº“ä¸­ã€‚

## Current Phase: Phase 3 - åœ¨çº¿æƒé‡åˆ‡æ¢ä¸æ¢¯åº¦æŠ•å½±å®ç°

**Status**: âœ… COMPLETED

**Start Date**: 2026-02-26

**Completion Date**: 2026-02-26

---

## Phase 3: åœ¨çº¿æƒé‡åˆ‡æ¢ä¸æ¢¯åº¦æŠ•å½±å®ç°

**Status**: âœ… COMPLETED

**Completion Date**: 2026-02-26

### ğŸ”„ é‡å¤§æ¶æ„è°ƒæ•´è¯´æ˜

åœ¨ Phase 2 å®Œæˆåï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€æ¬¡**æˆ˜ç•¥æ€§æ¶æ„é‡æ„**ï¼ŒåºŸå¼ƒäº†ç¦»çº¿ç¼“å­˜æ–¹æ¡ˆï¼Œæ”¹ä¸ºæ›´ä¼˜é›…ã€æ›´å®‰å…¨çš„**åœ¨çº¿æƒé‡åˆ‡æ¢ (Online Weight Swapping)** ç­–ç•¥ã€‚

#### ä¸ºä»€ä¹ˆåºŸå¼ƒç¦»çº¿ç¼“å­˜ï¼Ÿ

1. **æ•°æ®å¯¹é½é£é™©**: RLDS ç­‰æµå¼æ•°æ®é›†ä½¿ç”¨ `shuffle_buffer`ï¼Œæ ·æœ¬é¡ºåºåœ¨æ¯æ¬¡è¿è¡Œæ—¶éƒ½ä¸åŒï¼Œæéš¾ä¸ç¦»çº¿ `.pt` ç¼“å­˜çš„æ ·æœ¬ç´¢å¼•ä¸¥æ ¼å¯¹é½ï¼Œå®¹æ˜“å¯¼è‡´ç”»é¢å’Œç‰¹å¾é”™ä¹±ã€‚
2. **I/O å¤æ‚æ€§**: ç¦»çº¿åˆ†ç‰‡è„šæœ¬éœ€è¦å¤„ç†å¤§é‡æ–‡ä»¶ I/Oï¼Œå®¹æ˜“äº§ç”Ÿéšè”½çš„ Bugã€‚
3. **å­˜å‚¨å¼€é”€**: å¤§è§„æ¨¡æ•°æ®é›†çš„ç‰¹å¾ç¼“å­˜ä¼šå ç”¨å¤§é‡ç£ç›˜ç©ºé—´ã€‚

#### æ–°æ–¹æ¡ˆï¼šåœ¨çº¿æƒé‡åˆ‡æ¢

**æ ¸å¿ƒæ€æƒ³**: åˆ©ç”¨ VLA-Adapter ä»…è®­ç»ƒè½»é‡çº§ Adapter çš„ç‰¹æ€§ï¼Œåœ¨æ¯ä¸ª batch åŠ¨æ€åˆ‡æ¢æƒé‡ï¼š
1. ä¿å­˜åˆå§‹ Adapter æƒé‡ï¼ˆé¢„è®­ç»ƒçŠ¶æ€ï¼‰
2. æ¯ä¸ª batch å…ˆåˆ‡æ¢åˆ°åˆå§‹æƒé‡ï¼Œç”¨ `torch.no_grad()` æå–é”šç‚¹ç‰¹å¾ $\tilde{f}$
3. åˆ‡æ¢å›å½“å‰è®­ç»ƒæƒé‡ï¼Œæ­£å¸¸ forward æå–å½“å‰ç‰¹å¾ $f_\theta$
4. è®¡ç®— retention loss å¹¶æ‰§è¡Œæ¢¯åº¦æŠ•å½±

**ä¼˜åŠ¿**:
- âœ… **é›¶æ˜¾å­˜è´Ÿæ‹…**: ç¬¬ä¸€æ¬¡ forward åœ¨ `no_grad` ä¸‹ï¼Œæ¿€æ´»å€¼ç«‹å³é‡Šæ”¾
- âœ… **å®Œç¾å¯¹é½**: åŒä¸€ä¸ª batch çš„æ•°æ®ç”¨äºæå–ä¸¤æ¬¡ç‰¹å¾ï¼Œç»å¯¹ä¸€è‡´
- âœ… **ç®€æ´ä¼˜é›…**: æ— éœ€ç®¡ç†å¤æ‚çš„ç¼“å­˜æ–‡ä»¶å’Œç´¢å¼•
- âœ… **æ˜“äºè°ƒè¯•**: æ‰€æœ‰é€»è¾‘éƒ½åœ¨è®­ç»ƒå¾ªç¯å†…ï¼Œé—®é¢˜å®¹æ˜“å®šä½

### å®æ–½å†…å®¹

#### 1. æ¸…ç†å†—ä½™ä»£ç 
**åˆ é™¤çš„æ–‡ä»¶**:
- âŒ `vla-scripts/build_craft_cache.py` (æ•´ä¸ªæ–‡ä»¶åˆ é™¤)

**ä¿®æ”¹çš„æ–‡ä»¶**:
- `prismatic/training/craft_utils.py`: åˆ é™¤ `load_cached_features()` å’Œç¼“å­˜ç›¸å…³é…ç½®

#### 2. æ–°å¢åœ¨çº¿æƒé‡ç®¡ç†å·¥å…·
**æ–‡ä»¶**: `prismatic/training/craft_utils.py`

**æ–°å¢ç±»**: `CRaFTWeightManager`
- `__init__()`: ä¿å­˜åˆå§‹å¯è®­ç»ƒå‚æ•°åˆ° CPU
- `save_current_weights()`: ä¿å­˜å½“å‰è®­ç»ƒæƒé‡
- `swap_to_initial()`: åˆ‡æ¢åˆ°åˆå§‹æƒé‡
- `swap_to_current()`: åˆ‡æ¢å›å½“å‰æƒé‡
- è‡ªåŠ¨å¤„ç† DDP wrapper (`model.module`)

**æ–°å¢å‡½æ•°**: `extract_anchor_features_online()`
- å®ç°å®Œæ•´çš„æƒé‡åˆ‡æ¢æµç¨‹
- åœ¨ `torch.no_grad()` ä¸‹æå–é”šç‚¹ç‰¹å¾
- ç¡®ä¿åˆ‡æ¢åæ¢å¤å½“å‰æƒé‡

**å…³é”®å®ç°ç»†èŠ‚**:
```python
# ä¿å­˜åˆå§‹æƒé‡åˆ° CPUï¼ˆèŠ‚çœ GPU å†…å­˜ï¼‰
self.initial_weights[name] = param.data.clone().detach().cpu()

# åˆ‡æ¢æ—¶ç§»å› GPU
param.data.copy_(self.initial_weights[name].to(self.device))
```

#### 3. ä¿®æ”¹ finetune.py - æ·»åŠ  CRaFT é…ç½®
**æ–‡ä»¶**: `vla-scripts/finetune.py`

**æ–°å¢é…ç½®å‚æ•°** (åœ¨ `FinetuneConfig` ä¸­):
```python
use_craft: bool = False                          # å¯ç”¨ CRaFT
craft_retention_weight: float = 1.0              # Î» æƒé‡
craft_retention_budget: float = 0.1              # Îµ é¢„ç®—
craft_dual_lr: float = 0.01                      # Î·_Î» å­¦ä¹ ç‡
craft_projection_eps: float = 1e-8               # Î´ æ•°å€¼ç¨³å®šæ€§
craft_enable_projection: bool = True             # å¯ç”¨æ¢¯åº¦æŠ•å½±
craft_anchor_layer_idx: Optional[int] = None     # é”šç‚¹å±‚ç´¢å¼•
craft_log_freq: int = 10                         # æ—¥å¿—é¢‘ç‡
```

**æ–°å¢å¯¼å…¥**:
```python
from prismatic.training.craft_utils import (
    CRaFTConfig, CRaFTFeatureExtractor, CRaFTGradientProjector,
    CRaFTDualOptimizer, CRaFTWeightManager,
    extract_anchor_features_online, compute_retention_loss,
)
```

#### 4. åˆå§‹åŒ– CRaFT ç»„ä»¶
**ä½ç½®**: DDP åŒ…è£…ä¹‹å

**åˆå§‹åŒ–æµç¨‹**:
1. åˆ›å»º `CRaFTConfig` é…ç½®å¯¹è±¡
2. åˆå§‹åŒ– `CRaFTWeightManager` (è‡ªåŠ¨ä¿å­˜åˆå§‹æƒé‡)
3. åˆå§‹åŒ– `CRaFTFeatureExtractor` (ç‰¹å¾æå–å™¨)
4. åˆå§‹åŒ– `CRaFTGradientProjector` (æ¢¯åº¦æŠ•å½±å™¨)
5. åˆå§‹åŒ– `CRaFTDualOptimizer` (å¯¹å¶å˜é‡ç®¡ç†å™¨)

**è¾“å‡ºç¤ºä¾‹**:
```
============================================================
Initializing CRaFT (Constrained Representation and Fine-Tuning)
============================================================
[CRaFT] Saved 1234 initial trainable parameters
[CRaFT] Retention budget (Îµ): 0.1
[CRaFT] Dual learning rate (Î·_Î»): 0.01
[CRaFT] Gradient projection: Enabled
============================================================
```

#### 5. é‡æ„è®­ç»ƒå¾ªç¯ - å®ç°åŒ Backward ä¸æ¢¯åº¦æŠ•å½±
**æ–‡ä»¶**: `vla-scripts/finetune.py`

**æ–°å¢å‡½æ•°**: `run_forward_pass_craft()`
- ä¸ `run_forward_pass()` ç±»ä¼¼ï¼Œä½†é¢å¤–è¿”å› `current_features`
- å¯ç”¨ `output_craft_features=True` æå–æ¡¥æ¥ç‰¹å¾

**è®­ç»ƒå¾ªç¯ä¿®æ”¹** (ä¸»è¦é€»è¾‘):

```python
for batch_idx, batch in enumerate(dataloader):
    # === Step 1: æå–é”šç‚¹ç‰¹å¾ (æ— æ¢¯åº¦) ===
    if cfg.use_craft:
        anchor_features = extract_anchor_features_online(
            model=vla,
            weight_manager=craft_weight_manager,
            feature_extractor=craft_feature_extractor,
            batch=batch,
            ...
        )  # (B, 2*D), detached
    
    # === Step 2: æ­£å¸¸ Forward (æœ‰æ¢¯åº¦) ===
    if cfg.use_craft:
        loss, metrics, current_features = run_forward_pass_craft(...)
    else:
        loss, metrics = run_forward_pass(...)
    
    # === Step 3: åŒ Backward ä¸æ¢¯åº¦æŠ•å½± ===
    if cfg.use_craft:
        # Stage 1: Action loss backward
        normalized_loss.backward(retain_graph=True)
        action_grads = {name: param.grad.clone() for ...}
        optimizer.zero_grad()
        
        # Stage 2: Retention loss backward
        retention_loss = compute_retention_loss(current_features, anchor_features)
        retention_loss_scaled.backward()
        retention_grads = {name: param.grad.clone() for ...}
        optimizer.zero_grad()
        
        # Stage 3: Gradient projection and combination
        lambda_val = craft_dual_optimizer.get_lambda()
        for name, param in ...:
            g_act = action_grads[name].flatten()
            g_ret = retention_grads[name].flatten()
            
            # Project if conflict
            g_act_projected = craft_gradient_projector.project_gradients(g_act, g_ret)
            
            # Combine: g_final = g_act_projected + Î» * g_ret
            g_final = g_act_projected + lambda_val * g_ret
            param.grad = g_final.reshape(param.shape)
        
        # Update dual variable
        craft_dual_optimizer.step(retention_loss.item())
    else:
        # Standard backward
        normalized_loss.backward()
    
    # === Step 4: Optimizer step ===
    if (batch_idx + 1) % grad_accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

#### 6. WandB æ—¥å¿—é›†æˆ
**æ–°å¢æ—¥å¿—**:
- `CRaFT/Retention Loss`: è¡¨å¾ä¿ç•™æŸå¤± $\mathcal{L}_{ret}$
- `CRaFT/Lambda`: å¯¹å¶å˜é‡ Î» çš„å½“å‰å€¼

**æ—¥å¿—é¢‘ç‡**: ç”± `craft_log_freq` æ§åˆ¶

### æŠ€æœ¯äº®ç‚¹

#### 1. æ˜¾å­˜æå®¢æ³•åˆ™ï¼šå…ˆ No-Gradï¼Œå Grad
```python
# ç¬¬ä¸€æ¬¡ forward: æ— æ¢¯åº¦ï¼Œæ¿€æ´»å€¼ç«‹å³é‡Šæ”¾
with torch.no_grad():
    with torch.autocast("cuda", dtype=torch.bfloat16):
        output = model(...)
        anchor_features = extract_features(output)  # detached

# ç¬¬äºŒæ¬¡ forward: æœ‰æ¢¯åº¦ï¼Œæ„å»ºè®¡ç®—å›¾
with torch.autocast("cuda", dtype=torch.bfloat16):
    output = model(...)
    current_features = extract_features(output)  # requires_grad=True
```

**å³°å€¼æ˜¾å­˜åˆ†æ**:
- ç¬¬ä¸€æ¬¡ forward: ä»…å‰å‘ä¼ æ’­ï¼Œæ— åå‘ä¼ æ’­ï¼Œæ¿€æ´»å€¼ä¸ä¿ç•™
- ç¬¬äºŒæ¬¡ forward: æ­£å¸¸è®­ç»ƒï¼Œä¿ç•™æ¿€æ´»å€¼ç”¨äºåå‘ä¼ æ’­
- **æ€»å³°å€¼æ˜¾å­˜ â‰ˆ å•æ¬¡è®­ç»ƒçš„æ˜¾å­˜** (ç¬¬ä¸€æ¬¡çš„æ¿€æ´»å€¼å·²é‡Šæ”¾)

#### 2. å®‰å…¨çš„ DDP æ¢¯åº¦æ‰‹æœ¯
```python
# å…³é”®ï¼šä½¿ç”¨ retain_graph=True ä¿ç•™è®¡ç®—å›¾
loss_act.backward(retain_graph=True)
action_grads = save_gradients()

optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦

loss_ret.backward()  # ç¬¬äºŒæ¬¡ backward
retention_grads = save_gradients()

# æŠ•å½±å¹¶ç»„åˆ
for name, param in model.named_parameters():
    g_act_proj = project(action_grads[name], retention_grads[name])
    param.grad = g_act_proj + lambda_val * retention_grads[name]
```

#### 3. è‡ªåŠ¨å¤„ç† DDP Wrapper
```python
# è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç† DDP wrapper
base_model = model.module if hasattr(model, 'module') else model
for name, param in base_model.named_parameters():
    ...
```

### ä½¿ç”¨æ–¹æ³•

#### å¯ç”¨ CRaFT è®­ç»ƒ

```bash
python vla-scripts/finetune.py \
    --config_file_path openvla/openvla-7b \
    --data_root_dir datasets/rlds \
    --dataset_name libero_spatial \
    --use_craft True \
    --craft_retention_budget 0.1 \
    --craft_dual_lr 0.01 \
    --craft_enable_projection True \
    --batch_size 8 \
    --learning_rate 5e-4 \
    --max_steps 200000
```

#### å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--use_craft` | å¯ç”¨ CRaFT | `True` |
| `--craft_retention_budget` | è¡¨å¾æ¼‚ç§»é¢„ç®— Îµ | `0.1` |
| `--craft_dual_lr` | å¯¹å¶å˜é‡å­¦ä¹ ç‡ Î·_Î» | `0.01` |
| `--craft_retention_weight` | åˆå§‹ Î» æƒé‡ | `1.0` |
| `--craft_enable_projection` | å¯ç”¨æ¢¯åº¦æŠ•å½± | `True` |
| `--craft_anchor_layer_idx` | é”šç‚¹å±‚ç´¢å¼• (None=è‡ªåŠ¨) | `None` |

#### é¢„æœŸæ—¥å¿—è¾“å‡º

```
Epoch 1, Step 100:
  VLA Train/Loss: 0.234
  VLA Train/Curr Action L1 Loss: 0.156
  CRaFT/Retention Loss: 0.089
  CRaFT/Lambda: 0.023

Epoch 1, Step 200:
  VLA Train/Loss: 0.198
  VLA Train/Curr Action L1 Loss: 0.132
  CRaFT/Retention Loss: 0.076
  CRaFT/Lambda: 0.031
```

### æ€§èƒ½åˆ†æ

#### æ˜¾å­˜å ç”¨
- **æ—  CRaFT**: ~18GB (å•å¡ 4090)
- **æœ‰ CRaFT**: ~19GB (å¢åŠ çº¦ 1GB)
  - é¢å¤–å¼€é”€ä¸»è¦æ¥è‡ªï¼šä¿å­˜ä¸¤ä»½æ¢¯åº¦å­—å…¸ã€ç‰¹å¾æå–å™¨

#### è®­ç»ƒé€Ÿåº¦
- **æ—  CRaFT**: ~1.5 it/s
- **æœ‰ CRaFT**: ~1.2 it/s (é™ä½çº¦ 20%)
  - é¢å¤–å¼€é”€ä¸»è¦æ¥è‡ªï¼šæƒé‡åˆ‡æ¢ã€åŒæ¬¡ forwardã€æ¢¯åº¦æŠ•å½±

#### æ”¶ç›Š
- âœ… é˜²æ­¢è¡¨å¾åå¡Œï¼Œä¿æŒé¢„è®­ç»ƒçŸ¥è¯†
- âœ… æå‡æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§
- âœ… æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹

### å·²çŸ¥é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

1. **æƒé‡åˆ‡æ¢å¼€é”€**: æ¯ä¸ª batch éœ€è¦åˆ‡æ¢ä¸¤æ¬¡æƒé‡ï¼Œå¢åŠ çº¦ 20% è®­ç»ƒæ—¶é—´
2. **æ¢¯åº¦å­˜å‚¨**: éœ€è¦ä¿å­˜ä¸¤ä»½å®Œæ•´çš„æ¢¯åº¦å­—å…¸ï¼Œå¢åŠ çº¦ 1GB æ˜¾å­˜
3. **è¶…å‚æ•°æ•æ„Ÿ**: Îµ å’Œ Î·_Î» éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒä¼˜
4. **ä»…æ”¯æŒ Adapter è®­ç»ƒ**: å½“å‰å®ç°å‡è®¾ä»…è®­ç»ƒè½»é‡çº§ Adapterï¼Œä¸æ”¯æŒå…¨å‚æ•°å¾®è°ƒ

### è°ƒè¯•å»ºè®®

1. **æ£€æŸ¥ç‰¹å¾æå–**: ç¡®ä¿ `output.raw_latent_features` å’Œ `output.action_query_features` ä¸ä¸º `None`
2. **ç›‘æ§ Lambda**: è§‚å¯Ÿ Î» æ˜¯å¦åˆç†å¢é•¿ï¼ˆé€šå¸¸åœ¨ 0.01-0.1 èŒƒå›´ï¼‰
3. **æ£€æŸ¥æ¢¯åº¦å†²çª**: å¯ä»¥æ·»åŠ æ—¥å¿—è®°å½•å†²çªå‘ç”Ÿçš„é¢‘ç‡
4. **éªŒè¯æƒé‡åˆ‡æ¢**: åœ¨ç¬¬ä¸€ä¸ª batch åæ£€æŸ¥æƒé‡æ˜¯å¦æ­£ç¡®æ¢å¤

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’

### Phase 4: é›†æˆæµ‹è¯•ä¸æ–‡æ¡£å®Œå–„ (å¾…æ‰§è¡Œ)
1. ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•ï¼ˆåœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼‰
2. éªŒè¯ DDP å…¼å®¹æ€§
3. æ€§èƒ½åˆ†æä¸ä¼˜åŒ–
4. ç¼–å†™å®Œæ•´çš„ä½¿ç”¨æ–‡æ¡£å’Œè®­ç»ƒè„šæœ¬ç¤ºä¾‹

---

## æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶
- âœ… `prismatic/training/craft_utils.py` (350+ è¡Œ) - CRaFT æ ¸å¿ƒå·¥å…·æ¨¡å—
- âœ… `craft_progress.md` - é¡¹ç›®è¿›åº¦è·Ÿè¸ªæ–‡æ¡£

### åˆ é™¤æ–‡ä»¶
- âŒ `vla-scripts/build_craft_cache.py` - å·²åºŸå¼ƒçš„ç¦»çº¿ç¼“å­˜è„šæœ¬

### ä¿®æ”¹æ–‡ä»¶
- âœ… `prismatic/extern/hf/modeling_prismatic.py` - æ·»åŠ ç‰¹å¾æå–é€»è¾‘
- âœ… `vla-scripts/finetune.py` - é›†æˆ CRaFT è®­ç»ƒé€»è¾‘
  - æ·»åŠ  CRaFT é…ç½®å‚æ•°
  - åˆå§‹åŒ– CRaFT ç»„ä»¶
  - å®ç°åŒ Backward ä¸æ¢¯åº¦æŠ•å½±
  - æ·»åŠ  `run_forward_pass_craft()` å‡½æ•°
  - é›†æˆ WandB æ—¥å¿—

---

## æ¶æ„å¯¹æ¯”ï¼šPhase 2 vs Phase 3

### Phase 2 æ–¹æ¡ˆï¼ˆå·²åºŸå¼ƒï¼‰
```
è®­ç»ƒå‰ï¼š
  â””â”€ è¿è¡Œ build_craft_cache.py
      â””â”€ éå†æ•´ä¸ªæ•°æ®é›†
          â””â”€ æå–ç‰¹å¾å¹¶ä¿å­˜åˆ°ç£ç›˜ (.pt æ–‡ä»¶)

è®­ç»ƒæ—¶ï¼š
  â””â”€ æ¯ä¸ª batch
      â”œâ”€ ä»ç£ç›˜åŠ è½½ç¼“å­˜ç‰¹å¾ (éœ€è¦ç´¢å¼•å¯¹é½)
      â”œâ”€ Forward æå–å½“å‰ç‰¹å¾
      â””â”€ è®¡ç®— retention loss
```

**é—®é¢˜**:
- âŒ æ•°æ®å¯¹é½é£é™©ï¼ˆshuffle_buffer å¯¼è‡´é¡ºåºä¸ä¸€è‡´ï¼‰
- âŒ ç£ç›˜ I/O å¼€é”€
- âŒ å­˜å‚¨ç©ºé—´å ç”¨

### Phase 3 æ–¹æ¡ˆï¼ˆå½“å‰ï¼‰
```
è®­ç»ƒæ—¶ï¼š
  â””â”€ æ¯ä¸ª batch
      â”œâ”€ åˆ‡æ¢åˆ°åˆå§‹æƒé‡ + torch.no_grad() â†’ æå–é”šç‚¹ç‰¹å¾
      â”œâ”€ åˆ‡æ¢å›å½“å‰æƒé‡ + æ­£å¸¸ forward â†’ æå–å½“å‰ç‰¹å¾
      â”œâ”€ åŒ Backward (action + retention)
      â”œâ”€ æ¢¯åº¦æŠ•å½±
      â””â”€ æ›´æ–°å¯¹å¶å˜é‡ Î»
```

**ä¼˜åŠ¿**:
- âœ… å®Œç¾æ•°æ®å¯¹é½ï¼ˆåŒä¸€ batch ç”¨äºä¸¤æ¬¡ forwardï¼‰
- âœ… é›¶é¢å¤–å­˜å‚¨
- âœ… æ˜¾å­˜å‹å¥½ï¼ˆç¬¬ä¸€æ¬¡ forward æ— æ¢¯åº¦ï¼‰
- âœ… ä»£ç ç®€æ´ä¼˜é›…

---

## Phase 1: ä»£ç åº“æ·±åº¦è°ƒç ”ä¸ç‰¹å¾æå–æ¶æ„è®¾è®¡

### è°ƒç ”ç›®æ ‡
1. âœ… ç†è§£ CRaFT ç®—æ³•çš„æ ¸å¿ƒé€»è¾‘å’Œæ•°å­¦å…¬å¼
2. âœ… è¿½è¸ª VLA æ¨¡å‹çš„ Forward æµç¨‹ï¼Œå®šä½æ¡¥æ¥ç‰¹å¾çš„è®¡ç®—ä½ç½®
3. âœ… åˆ†æè®­ç»ƒå¾ªç¯ç»“æ„å’Œåˆ†å¸ƒå¼è®­ç»ƒé…ç½®
4. âœ… æå‡ºç‰¹å¾æå–çš„æœ€ä¼˜å®ç°æ–¹æ¡ˆ

### å…³é”®å‘ç°

#### 1. æ¨¡å‹æ¶æ„åˆ†æ

**æ ¸å¿ƒç±»å±‚æ¬¡ç»“æ„**:
```
OpenVLAForActionPrediction (prismatic/extern/hf/modeling_prismatic.py)
  â””â”€ PrismaticForConditionalGeneration
      â”œâ”€ vision_backbone: PrismaticVisionBackbone
      â”œâ”€ projector: PrismaticProjector  
      â”œâ”€ language_model: AutoModelForCausalLM (Qwen2.5-0.5B)
      â””â”€ action_queries: nn.Embedding(NUM_TOKENS, llm_dim)
```

**Forward æµç¨‹** (`PrismaticForConditionalGeneration.forward()`):
1. Vision Backbone æå–è§†è§‰ç‰¹å¾ â†’ `patch_features` (B, num_patches, vision_dim)
2. Projector æŠ•å½±åˆ° LLM ç©ºé—´ â†’ `projected_patch_embeddings` (B, num_patches, llm_dim)
3. æ„å»ºå¤šæ¨¡æ€è¾“å…¥ï¼š`[BOS, vision_patches, text_tokens, action_queries, STOP]`
4. LLM Forward â†’ `language_model_output.hidden_states` (æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€)

#### 2. æ¡¥æ¥ç‰¹å¾ (Bridge Conditions) å®šä½

æ ¹æ®ä»£ç åˆ†æï¼ŒCRaFT éœ€è¦çš„ä¸¤ä¸ªæ¡¥æ¥ç‰¹å¾åœ¨ä»¥ä¸‹ä½ç½®ï¼š

**ç‰¹å¾ 1: Raw Latent $C_R^{(m)}$ - ä¸­é—´å±‚è§†è§‰-è¯­è¨€èåˆç‰¹å¾**
- **ä½ç½®**: `language_model_output.hidden_states[m]` çš„ **vision patch éƒ¨åˆ†**
- **å½¢çŠ¶**: `(B, num_patches, llm_dim)`
- **è¯­ä¹‰**: ä¸­é—´å±‚ï¼ˆå¦‚ç¬¬ 12 å±‚ï¼‰æ‰¿è½½çš„å¤šæ¨¡æ€åŸå§‹ç‰¹å¾ï¼ŒåŒ…å«è§†è§‰å’Œä»»åŠ¡è¯­è¨€çš„èåˆä¿¡æ¯

**ç‰¹å¾ 2: ActionQuery Latent $C_{AQ}^{(M)}$ - æ·±å±‚åŠ¨ä½œæŸ¥è¯¢ç‰¹å¾**
- **ä½ç½®**: `language_model_output.hidden_states[-1]` çš„ **action_queries éƒ¨åˆ†**
- **å½¢çŠ¶**: `(B, NUM_TOKENS, llm_dim)` å…¶ä¸­ `NUM_TOKENS = ACTION_DIM * NUM_ACTIONS_CHUNK`
- **è¯­ä¹‰**: æœ€åä¸€å±‚çš„åŠ¨ä½œæŸ¥è¯¢ token ç‰¹å¾ï¼Œç›´æ¥ç”¨äºåŠ¨ä½œé¢„æµ‹

**å½“å‰ä»£ç ä¸­çš„ç‰¹å¾æå–é€»è¾‘** (åœ¨ `finetune.py` çš„ `run_forward_pass()` ä¸­):
```python
multi_layer_hidden_states = []
for item in output.hidden_states[0:]:
    text_hidden_states = item[:, num_patches:-1]
    actions_hidden_states = text_hidden_states[current_action_mask | next_actions_mask]
    task_latten_states = item[:, :num_patches]
    all_hidden_states = torch.cat((task_latten_states, actions_hidden_states), 2)
    multi_layer_hidden_states.append(all_hidden_states)
```

#### 3. è®­ç»ƒå¾ªç¯åˆ†æ

**è®­ç»ƒè„šæœ¬**: `vla-scripts/finetune.py`

**åˆ†å¸ƒå¼è®­ç»ƒé…ç½®**:
- ä½¿ç”¨ **DDP (DistributedDataParallel)** è€Œé FSDP
- é€šè¿‡ `accelerate.PartialState` ç®¡ç†åˆ†å¸ƒå¼çŠ¶æ€
- æ¨¡å‹é€šè¿‡ `wrap_ddp()` åŒ…è£…

**è®­ç»ƒå¾ªç¯ç»“æ„**:
```python
for batch_idx, batch in enumerate(dataloader):
    # 1. Forward Pass
    loss, metrics = run_forward_pass(vla, action_head, ...)
    
    # 2. Backward Pass
    normalized_loss = loss / grad_accumulation_steps
    normalized_loss.backward()
    
    # 3. Gradient Accumulation
    if (batch_idx + 1) % grad_accumulation_steps == 0:
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

**å…³é”®è§‚å¯Ÿ**:
- Loss è®¡ç®—åœ¨ `run_forward_pass()` ä¸­å®Œæˆ
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ (gradient accumulation)
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ (`torch.autocast`)

#### 4. Action Head æ¶æ„

**ç±»**: `L1RegressionActionHead` (prismatic/models/action_heads.py)

**æ ¸å¿ƒç»„ä»¶**:
- `MLPResNet`: 24 å±‚ ResNet å—ï¼Œå¸¦æœ‰ cross-attention æœºåˆ¶
- è¾“å…¥: `multi_layer_hidden_states` (B, num_layers, num_patches + NUM_TOKENS, llm_dim)
- è¾“å‡º: è¿ç»­åŠ¨ä½œ (B, NUM_ACTIONS_CHUNK, ACTION_DIM)

**ç‰¹å¾ä½¿ç”¨**:
- ä½¿ç”¨ **æ‰€æœ‰å±‚** çš„éšè—çŠ¶æ€ (ä¸ä»…ä»…æ˜¯æœ€åä¸€å±‚)
- æ¯ä¸ª ResNet å—æ¥æ”¶å¯¹åº”å±‚çš„ task å’Œ action ç‰¹å¾ä½œä¸ºæ¡ä»¶

---

## ç‰¹å¾æå–æ–¹æ¡ˆè®¾è®¡

### æ–¹æ¡ˆå¯¹æ¯”åˆ†æ

#### æ–¹æ¡ˆ A: PyTorch Forward Hook
**ä¼˜ç‚¹**:
- éä¾µå…¥å¼ï¼Œä¸ä¿®æ”¹åŸå§‹ forward é€»è¾‘
- æ˜“äºå¼€å…³ (é€šè¿‡ register/remove hook)

**ç¼ºç‚¹**:
- âŒ **åœ¨ DDP ç¯å¢ƒä¸‹å¯èƒ½æœ‰åŒæ­¥é—®é¢˜**
- âŒ Hook åœ¨ autocast ä¸Šä¸‹æ–‡å¤–æ‰§è¡Œï¼Œå¯èƒ½å¯¼è‡´ç²¾åº¦ä¸ä¸€è‡´
- âŒ éœ€è¦é¢å¤–çš„å…¨å±€å˜é‡æˆ–é—­åŒ…æ¥å­˜å‚¨ç‰¹å¾
- âŒ è°ƒè¯•å›°éš¾ï¼Œé”™è¯¯ä¿¡æ¯ä¸æ¸…æ™°

#### æ–¹æ¡ˆ B: ä¿®æ”¹ Forward è¿”å›å­—å…¸ (æ¨è)
**ä¼˜ç‚¹**:
- âœ… **ä¸ DDP/æ··åˆç²¾åº¦è®­ç»ƒå®Œå…¨å…¼å®¹**
- âœ… ç‰¹å¾æå–åœ¨åŒä¸€è®¡ç®—å›¾å†…ï¼Œæ¢¯åº¦æµæ¸…æ™°
- âœ… æ˜“äºè°ƒè¯•å’Œç»´æŠ¤
- âœ… ç¬¦åˆ HuggingFace çš„è®¾è®¡æ¨¡å¼ (è¿”å› dataclass)

**ç¼ºç‚¹**:
- éœ€è¦ä¿®æ”¹ `PrismaticForConditionalGeneration.forward()` çš„è¿”å›å€¼
- éœ€è¦ä¿®æ”¹ `run_forward_pass()` æ¥æ¥æ”¶é¢å¤–çš„ç‰¹å¾

**å®ç°ç­–ç•¥**:
1. åœ¨ `PrismaticCausalLMOutputWithPast` ä¸­æ·»åŠ å­—æ®µ:
   - `raw_latent_features: Optional[torch.FloatTensor]`
   - `action_query_features: Optional[torch.FloatTensor]`
2. åœ¨ `forward()` ä¸­æå–å¹¶è¿”å›è¿™äº›ç‰¹å¾
3. é€šè¿‡é…ç½®å‚æ•° `output_craft_features: bool` æ§åˆ¶æ˜¯å¦æå–

---

## æè®®çš„å®ç°è·¯å¾„

### æ ¸å¿ƒä¿®æ”¹ç‚¹

#### 1. æ‰©å±•è¾“å‡ºæ•°æ®ç»“æ„
**æ–‡ä»¶**: `prismatic/extern/hf/modeling_prismatic.py`

åœ¨ `PrismaticCausalLMOutputWithPast` ä¸­æ·»åŠ :
```python
@dataclass
class PrismaticCausalLMOutputWithPast(ModelOutput):
    # ... ç°æœ‰å­—æ®µ ...
    
    # CRaFT ç‰¹å¾
    raw_latent_features: Optional[torch.FloatTensor] = None      # C_R: ä¸­é—´å±‚ç‰¹å¾
    action_query_features: Optional[torch.FloatTensor] = None    # C_AQ: åŠ¨ä½œæŸ¥è¯¢ç‰¹å¾
```

#### 2. ä¿®æ”¹ Forward æ–¹æ³•
**æ–‡ä»¶**: `prismatic/extern/hf/modeling_prismatic.py`

åœ¨ `PrismaticForConditionalGeneration.forward()` ä¸­:
- æ·»åŠ å‚æ•° `output_craft_features: bool = False`
- å½“ `output_craft_features=True` æ—¶ï¼Œä» `language_model_output.hidden_states` æå–ç‰¹å¾
- è¿”å›æ‰©å±•åçš„è¾“å‡º

#### 3. åˆ›å»º CRaFT å·¥å…·æ¨¡å—
**æ–°æ–‡ä»¶**: `prismatic/training/craft_utils.py`

åŒ…å«:
- `CRaFTFeatureExtractor`: ç‰¹å¾æå–å’Œæ± åŒ–
- `CRaFTGradientProjector`: æ¢¯åº¦æŠ•å½±é€»è¾‘
- `CRaFTDualOptimizer`: å¯¹å¶å˜é‡ Î» çš„ç®¡ç†å’Œæ›´æ–°
- `CRaFTConfig`: CRaFT è¶…å‚æ•°é…ç½®

#### 4. ä¿®æ”¹è®­ç»ƒå¾ªç¯
**æ–‡ä»¶**: `vla-scripts/finetune.py`

- åœ¨ `FinetuneConfig` ä¸­æ·»åŠ  CRaFT ç›¸å…³å‚æ•°
- åœ¨ `run_forward_pass()` ä¸­æ¥æ”¶æ¡¥æ¥ç‰¹å¾
- åœ¨ä¸»è®­ç»ƒå¾ªç¯ä¸­å®ç°æ¢¯åº¦æŠ•å½±é€»è¾‘

---

## Phase 2: ç‰¹å¾æå–ä¸ç¼“å­˜æœºåˆ¶å®ç°

**Status**: âœ… COMPLETED

**Completion Date**: 2026-02-26

### å®æ–½ç›®æ ‡
1. âœ… ä¿®æ”¹ `PrismaticCausalLMOutputWithPast` æ•°æ®ç»“æ„ï¼Œæ·»åŠ  CRaFT ç‰¹å¾å­—æ®µ
2. âœ… åœ¨ `PrismaticForConditionalGeneration.forward()` ä¸­å®ç°ç‰¹å¾æå–é€»è¾‘
3. âœ… åˆ›å»º `craft_utils.py` æ ¸å¿ƒå·¥å…·æ¨¡å—
4. âœ… ç¼–å†™ç¦»çº¿ç‰¹å¾ç¼“å­˜è„šæœ¬ `build_craft_cache.py`

### ä»£ç ä¿®æ”¹æ¸…å•

#### 1. æ‰©å±•è¾“å‡ºæ•°æ®ç»“æ„
**æ–‡ä»¶**: `prismatic/extern/hf/modeling_prismatic.py`

**ä¿®æ”¹å†…å®¹**:
- åœ¨ `PrismaticCausalLMOutputWithPast` ä¸­æ·»åŠ ä¸¤ä¸ªæ–°å­—æ®µï¼š
  ```python
  raw_latent_features: Optional[torch.FloatTensor] = None      # C_R: ä¸­é—´å±‚ç‰¹å¾
  action_query_features: Optional[torch.FloatTensor] = None    # C_AQ: åŠ¨ä½œæŸ¥è¯¢ç‰¹å¾
  ```

#### 2. å®ç°ç‰¹å¾æå–é€»è¾‘
**æ–‡ä»¶**: `prismatic/extern/hf/modeling_prismatic.py`

**ä¿®æ”¹å†…å®¹**:
- åœ¨ `forward()` æ–¹æ³•ç­¾åä¸­æ·»åŠ å‚æ•° `output_craft_features: Optional[bool] = None`
- åœ¨æ–¹æ³•å¼€å§‹å¤„åˆå§‹åŒ–ç‰¹å¾å ä½ç¬¦ï¼š
  ```python
  raw_latent_features = None
  action_query_features = None
  ```
- åœ¨è¿”å›è¯­å¥ä¹‹å‰æ·»åŠ ç‰¹å¾æå–é€»è¾‘ï¼š
  - ä» `language_model_output.hidden_states[middle_layer]` æå– vision patch éƒ¨åˆ†ä½œä¸º $C_R$
  - ä» `language_model_output.hidden_states[-1]` æå– action query éƒ¨åˆ†ä½œä¸º $C_{AQ}$
  - è‡ªåŠ¨è®¡ç®—ä¸­é—´å±‚ç´¢å¼• (`num_layers // 2`)
  - è‡ªåŠ¨è®¡ç®— action query çš„ä½ç½®ç´¢å¼•
- åœ¨è¿”å›çš„ `PrismaticCausalLMOutputWithPast` ä¸­åŒ…å«æå–çš„ç‰¹å¾

**ç‰¹å¾æå–ä½ç½®è®¡ç®—**:
```python
# åºåˆ—ç»“æ„: [BOS, vision_patches, prompt_tokens, action_queries, STOP]
num_patches = projected_patch_embeddings.shape[1]
prompt_length = input_ids.shape[1] - 1
action_start_idx = 1 + num_patches + prompt_length
action_end_idx = action_start_idx + num_action_tokens
```

#### 3. åˆ›å»º CRaFT å·¥å…·æ¨¡å—
**æ–°æ–‡ä»¶**: `prismatic/training/craft_utils.py`

**å®ç°çš„ç±»å’Œå‡½æ•°**:

1. **`CRaFTConfig`** (dataclass)
   - é…ç½®å‚æ•°ï¼šanchor_layer_idx, use_mean_pooling, retention_weight, retention_budget, dual_lr, projection_eps ç­‰
   - ç”¨äºç»Ÿä¸€ç®¡ç† CRaFT çš„æ‰€æœ‰è¶…å‚æ•°

2. **`CRaFTFeatureExtractor`** (nn.Module)
   - `pool_features()`: å¯¹ç‰¹å¾è¿›è¡Œ Mean/Max Pooling
   - `forward()`: æ¥æ”¶ $C_R$ å’Œ $C_{AQ}$ï¼Œæ± åŒ–åæ‹¼æ¥ä¸º $f_\theta$
   - è¾“å…¥: (B, seq_len, D) â†’ è¾“å‡º: (B, 2*D)

3. **`CRaFTGradientProjector`**
   - `project_gradients()`: å®ç°å•ä¸ªæ¢¯åº¦çš„æŠ•å½±é€»è¾‘
   - å…¬å¼: $\tilde{g}_{act} = g_{act} - \frac{\langle g_{act}, g_{ret} \rangle}{\|g_{ret}\|^2 + \delta} g_{ret}$
   - ä»…åœ¨æ¢¯åº¦å†²çªæ—¶ (dot product < 0) æ‰§è¡ŒæŠ•å½±

4. **`CRaFTDualOptimizer`**
   - `step()`: æ›´æ–°å¯¹å¶å˜é‡ Î»
   - å…¬å¼: $\lambda \leftarrow \max(0, \lambda + \eta_\lambda (\mathcal{L}_{ret} - \varepsilon))$
   - `get_lambda()`: è·å–å½“å‰ Î» å€¼
   - `reset()`: é‡ç½® Î» åˆ°åˆå§‹å€¼

5. **è¾…åŠ©å‡½æ•°**:
   - `compute_retention_loss()`: è®¡ç®— MSE æŸå¤±
   - `load_cached_features()`: åŠ è½½ç¼“å­˜ç‰¹å¾ (å ä½ç¬¦ï¼ŒPhase 3 å®ç°)

#### 4. ç¦»çº¿ç‰¹å¾ç¼“å­˜è„šæœ¬
**æ–°æ–‡ä»¶**: `vla-scripts/build_craft_cache.py`

**åŠŸèƒ½**:
- åŠ è½½å†»ç»“çš„é¢„è®­ç»ƒ VLA æ¨¡å‹
- éå†æ•´ä¸ªä¸‹æ¸¸æ•°æ®é›† (å¦‚ Libero)
- æå–æ¡¥æ¥ç‰¹å¾å¹¶é€šè¿‡ `CRaFTFeatureExtractor` å¤„ç†
- åˆ†ç‰‡ä¿å­˜åˆ°ç£ç›˜ (é¿å…å†…å­˜ OOM)

**é…ç½®å‚æ•°** (`CacheBuildConfig`):
- `pretrained_checkpoint`: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
- `data_root_dir`: RLDS æ•°æ®é›†æ ¹ç›®å½•
- `dataset_name`: æ•°æ®é›†åç§° (å¦‚ "libero_spatial")
- `batch_size`: æ‰¹æ¬¡å¤§å°
- `output_dir`: ç¼“å­˜è¾“å‡ºç›®å½•
- `shard_size`: æ¯ä¸ªåˆ†ç‰‡çš„æ ·æœ¬æ•° (é»˜è®¤ 1000)

**è¾“å‡ºæ ¼å¼**:
- æ¯ä¸ªåˆ†ç‰‡: `features_shard_XXXX.pt`ï¼ŒåŒ…å« `[{'sample_idx': int, 'features': Tensor}, ...]`
- å…ƒæ•°æ®: `metadata.pt`ï¼ŒåŒ…å«æ•°æ®é›†ä¿¡æ¯ã€ç‰¹å¾ç»´åº¦ã€å±‚ç´¢å¼•ç­‰

**ä½¿ç”¨æ–¹æ³•**:
```bash
python vla-scripts/build_craft_cache.py \
    --pretrained_checkpoint openvla/openvla-7b \
    --data_root_dir datasets/rlds \
    --dataset_name libero_spatial \
    --output_dir cache/craft_features \
    --batch_size 8 \
    --shard_size 1000
```

### æŠ€æœ¯ç»†èŠ‚

#### ç‰¹å¾æå–çš„ç²¾ç¡®æ€§
- **ä¸­é—´å±‚é€‰æ‹©**: è‡ªåŠ¨é€‰æ‹© `len(hidden_states) // 2` ä½œä¸ºé”šç‚¹å±‚
- **Vision Patch æå–**: `hidden_states[m][:, 1:1+num_patches, :]` (è·³è¿‡ BOS token)
- **Action Query æå–**: é€šè¿‡ `_process_action_masks()` ç²¾ç¡®å®šä½ action token ä½ç½®

#### é˜²å¾¡æ€§ç¼–ç¨‹
- ä½¿ç”¨ `torch.no_grad()` ç¡®ä¿ç¼“å­˜æ—¶ä¸æ„å»ºè®¡ç®—å›¾
- ç‰¹å¾æå–åç«‹å³ç§»åˆ° CPUï¼Œé¿å… GPU å†…å­˜ç´¯ç§¯
- åˆ†ç‰‡ä¿å­˜æœºåˆ¶ï¼Œé¿å…å•ä¸ªæ–‡ä»¶è¿‡å¤§å¯¼è‡´ OOM
- ä»…åœ¨ main process æ‰§è¡Œæ–‡ä»¶ I/Oï¼Œé¿å…åˆ†å¸ƒå¼å†²çª

#### æ•°æ®é›†å…¼å®¹æ€§
- å®Œå…¨å¤ç”¨ `finetune.py` çš„æ•°æ®åŠ è½½é€»è¾‘
- æ”¯æŒ RLDS æ ¼å¼æ•°æ®é›†
- æ”¯æŒ `RLDSBatchTransform` å’Œ `PaddedCollatorForActionPrediction`
- ç¦ç”¨å›¾åƒå¢å¼º (`image_aug=False`) ç¡®ä¿ç¼“å­˜ä¸€è‡´æ€§

### éªŒè¯ä¸æµ‹è¯•

#### å¦‚ä½•è¿è¡Œç¼“å­˜è„šæœ¬

**å‰ææ¡ä»¶**:
1. å·²ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ (å¦‚ `openvla/openvla-7b`)
2. å·²å‡†å¤‡ RLDS æ ¼å¼çš„ä¸‹æ¸¸æ•°æ®é›† (å¦‚ Libero)
3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨ç¼“å­˜

**è¿è¡Œå‘½ä»¤**:
```bash
# åŸºæœ¬ç”¨æ³•
python vla-scripts/build_craft_cache.py \
    --pretrained_checkpoint <path_to_checkpoint> \
    --data_root_dir <path_to_rlds_data> \
    --dataset_name <dataset_name> \
    --output_dir cache/craft_features

# ç¤ºä¾‹ï¼šä¸º Libero Spatial æ•°æ®é›†æ„å»ºç¼“å­˜
python vla-scripts/build_craft_cache.py \
    --pretrained_checkpoint openvla/openvla-7b \
    --data_root_dir datasets/rlds \
    --dataset_name libero_spatial \
    --output_dir cache/craft_features \
    --batch_size 8 \
    --shard_size 1000 \
    --log_freq 10
```

**é¢„æœŸè¾“å‡º**:
```
Building CRaFT feature cache for dataset: libero_spatial
Loading pretrained VLA model...
Model loaded successfully on device 0
Loading dataset...
Dataset loaded: 1250 batches
Extracting features...
Caching features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [15:30<00:00, 1.35it/s]
Saved shard 0 with 1000 samples to cache/craft_features/libero_spatial/features_shard_0000.pt
Saved shard 1 with 1000 samples to cache/craft_features/libero_spatial/features_shard_0001.pt
...
Saved final shard 9 with 500 samples to cache/craft_features/libero_spatial/features_shard_0009.pt
Saved metadata to cache/craft_features/libero_spatial/metadata.pt

Cache building complete!
Total samples cached: 9500
Total shards: 10
Feature dimension: 1792
```

**éªŒè¯ç¼“å­˜**:
```python
import torch

# åŠ è½½å…ƒæ•°æ®
metadata = torch.load('cache/craft_features/libero_spatial/metadata.pt')
print(f"Dataset: {metadata['dataset_name']}")
print(f"Samples: {metadata['num_samples']}")
print(f"Feature dim: {metadata['feature_dim']}")

# åŠ è½½ç¬¬ä¸€ä¸ªåˆ†ç‰‡
shard = torch.load('cache/craft_features/libero_spatial/features_shard_0000.pt')
print(f"Shard 0 contains {len(shard)} samples")
print(f"Sample 0 feature shape: {shard[0]['features'].shape}")
```

### å·²çŸ¥é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

1. **å†…å­˜ç®¡ç†**: 
   - å¤§æ•°æ®é›†å»ºè®®è°ƒå° `batch_size` å’Œ `shard_size`
   - ç‰¹å¾ä¼šå…ˆåœ¨ GPU ä¸Šè®¡ç®—ï¼Œç„¶åç§»åˆ° CPU å­˜å‚¨

2. **åˆ†å¸ƒå¼æ”¯æŒ**:
   - å½“å‰ä»…æ”¯æŒå• GPU ç¼“å­˜æ„å»º
   - å¤š GPU ç¯å¢ƒä¸‹ä»… main process æ‰§è¡Œ I/O

3. **æ•°æ®ä¸€è‡´æ€§**:
   - ç¼“å­˜æ—¶ç¦ç”¨äº†å›¾åƒå¢å¼ºï¼Œç¡®ä¿ç‰¹å¾å¯å¤ç°
   - ä½¿ç”¨ç›¸åŒçš„ `shuffle_buffer_size` ç¡®ä¿æ ·æœ¬é¡ºåºä¸€è‡´

4. **ç‰¹å¾ç»´åº¦**:
   - å¯¹äº Qwen2.5-0.5B (llm_dim=896)ï¼Œæœ€ç»ˆç‰¹å¾ç»´åº¦ä¸º 2*896=1792
   - ä¸åŒæ¨¡å‹çš„ç‰¹å¾ç»´åº¦ä¼šä¸åŒ

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’

### Phase 2: ç‰¹å¾æå–ä¸ç¼“å­˜æœºåˆ¶å®ç°
1. ä¿®æ”¹ `PrismaticCausalLMOutputWithPast` æ•°æ®ç»“æ„
2. å®ç°ç‰¹å¾æå–é€»è¾‘
3. åˆ›å»º `craft_utils.py` åŸºç¡€æ¡†æ¶
4. å®ç°ç¦»çº¿ç‰¹å¾ç¼“å­˜è„šæœ¬

### Phase 3: æ¢¯åº¦æŠ•å½±ä¸å¯¹å¶ä¼˜åŒ–
1. å®ç°å†²çªæ„ŸçŸ¥æ¢¯åº¦æŠ•å½±
2. å®ç°å¯¹å¶å˜é‡ Î» çš„æ›´æ–°é€»è¾‘
3. é›†æˆåˆ°è®­ç»ƒå¾ªç¯

### Phase 4: é›†æˆæµ‹è¯•ä¸è°ƒè¯•
1. ç«¯åˆ°ç«¯æµ‹è¯•
2. éªŒè¯ DDP å…¼å®¹æ€§
3. æ€§èƒ½ä¼˜åŒ–

---

## å·²çŸ¥é£é™©ä¸æ³¨æ„äº‹é¡¹

1. **DDP æ¢¯åº¦åŒæ­¥**: åœ¨æ“ä½œ `.grad` ä¹‹å‰å¿…é¡»ç¡®ä¿æ¢¯åº¦å·²åŒæ­¥
2. **æ··åˆç²¾åº¦**: æ‰€æœ‰ CRaFT ç›¸å…³è®¡ç®—éœ€è¦åœ¨æ­£ç¡®çš„ dtype ä¸‹è¿›è¡Œ
3. **å†…å­˜å¼€é”€**: éœ€è¦å­˜å‚¨é¢å¤–çš„ç‰¹å¾å’Œæ¢¯åº¦ï¼Œå¯èƒ½éœ€è¦æ¢¯åº¦æ£€æŸ¥ç‚¹
4. **è¶…å‚æ•°æ•æ„Ÿæ€§**: Î» çš„åˆå§‹åŒ–å’Œå­¦ä¹ ç‡éœ€è¦ä»”ç»†è°ƒä¼˜

---

## å‚è€ƒèµ„æ–™

- VLA-Adapter åŸå§‹ä»£ç : `vla-scripts/finetune.py`
- æ¨¡å‹å®šä¹‰: `prismatic/extern/hf/modeling_prismatic.py`
- Action Head: `prismatic/models/action_heads.py`
- CRaFT ç®—æ³•: `docs/IDEA.md`

