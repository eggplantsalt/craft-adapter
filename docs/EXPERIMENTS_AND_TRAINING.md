# è®­ç»ƒä¸è¯„ä¼°å®Œå…¨æŒ‡å— (Training and Evaluation Guide)

æœ¬æ–‡æ¡£æä¾› VLA-Adapter + CRaFT è®­ç»ƒçš„å®Œæ•´æŒ‡å—ï¼ŒåŒ…æ‹¬è®­ç»ƒé…ç½®ã€æŒ‡æ ‡è§£è¯»ã€å®éªŒè„šæœ¬ä½¿ç”¨å’Œå¸¸è§é—®é¢˜æ’æŸ¥ã€‚

---

## ğŸ“‹ ç›®å½•

1. [è®­ç»ƒé…ç½®è¯¦è§£](#training-config)
2. [Baseline vs CRaFT å¯¹æ¯”](#baseline-vs-craft)
3. [æ ¸å¿ƒæŒ‡æ ‡æ·±åº¦è§£è¯»](#metrics-explained)
4. [è‡ªåŠ¨åŒ–å®éªŒè„šæœ¬ä½¿ç”¨](#experiment-scripts)
5. [è®­ç»ƒç›‘æ§ä¸è°ƒè¯•](#monitoring-debugging)
6. [å¸¸è§æŠ¥é”™ä¸æ’æŸ¥](#troubleshooting)

---

## <a name="training-config"></a>1. è®­ç»ƒé…ç½®è¯¦è§£

### 1.1 åŸºç¡€è®­ç»ƒå‚æ•°

```bash
python vla-scripts/finetune.py \
    --config_file_path "openvla/openvla-7b" \      # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    --dataset_name "libero_spatial_no_noops" \     # æ•°æ®é›†åç§°
    --batch_size 8 \                               # æ¯ GPU çš„ Batch Size
    --learning_rate 5e-4 \                         # å­¦ä¹ ç‡
    --max_steps 20000 \                            # æœ€å¤§è®­ç»ƒæ­¥æ•°
    --grad_accumulation_steps 1 \                  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    --save_freq 5000 \                             # Checkpoint ä¿å­˜é¢‘ç‡
    --wandb_project "vla-experiments"              # WandB é¡¹ç›®åç§°
```

**å‚æ•°è¯´æ˜**ï¼š

- **`config_file_path`**ï¼šé¢„è®­ç»ƒ VLA æ¨¡å‹çš„è·¯å¾„
  - å¯ä»¥æ˜¯ HuggingFace Hub ä¸Šçš„æ¨¡å‹ IDï¼ˆå¦‚ `openvla/openvla-7b`ï¼‰
  - ä¹Ÿå¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„ï¼ˆå¦‚ `runs/previous-experiment--10000_chkpt`ï¼‰

- **`batch_size`**ï¼šå• GPU çš„æ‰¹æ¬¡å¤§å°
  - æ€»æ‰¹æ¬¡å¤§å° = `batch_size Ã— num_gpus Ã— grad_accumulation_steps`
  - æ¨èå€¼ï¼š8-16ï¼ˆå–å†³äº GPU æ˜¾å­˜ï¼‰

- **`learning_rate`**ï¼šä¼˜åŒ–å™¨å­¦ä¹ ç‡
  - Baseline æ¨èï¼š5e-4
  - CRaFT æ¨èï¼š5e-4ï¼ˆä¸ Baseline ä¿æŒä¸€è‡´ï¼‰

- **`max_steps`**ï¼šæœ€å¤§è®­ç»ƒæ­¥æ•°
  - LIBERO æ•°æ®é›†æ¨èï¼š20000-50000 æ­¥
  - Few-Shot åœºæ™¯å¯é€‚å½“å‡å°‘

### 1.2 åŠ¨ä½œè¡¨ç¤ºé…ç½®

```bash
--use_l1_regression True \     # ä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´ + L1 å›å½’
--use_diffusion False          # ä¸ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆCRaFT æš‚ä¸æ”¯æŒï¼‰
```

**è¯´æ˜**ï¼š
- CRaFT ç›®å‰ä»…æ”¯æŒ **L1 å›å½’** çš„è¿ç»­åŠ¨ä½œè¡¨ç¤º
- ç¦»æ•£åŠ¨ä½œè¡¨ç¤ºï¼ˆNext-Token Predictionï¼‰å’Œæ‰©æ•£æ¨¡å‹æš‚ä¸æ”¯æŒ

### 1.3 æ•°æ®å¢å¼ºé…ç½®

```bash
--image_aug True \             # å¯ç”¨å›¾åƒå¢å¼ºï¼ˆå¼ºçƒˆæ¨èï¼‰
--shuffle_buffer_size 100000   # æ•°æ®æ‰“ä¹±ç¼“å†²åŒºå¤§å°
```

**å›¾åƒå¢å¼ºçš„ä½œç”¨**ï¼š
- æå‡æ¨¡å‹å¯¹è§†è§‰å˜åŒ–çš„é²æ£’æ€§
- é˜²æ­¢è¿‡æ‹Ÿåˆ
- **å¼ºçƒˆæ¨èä¿æŒå¯ç”¨**

---

## <a name="baseline-vs-craft"></a>2. Baseline vs CRaFT å¯¹æ¯”

### 2.1 Baseline è®­ç»ƒï¼ˆæ ‡å‡† VLA å¾®è°ƒï¼‰

```bash
python vla-scripts/finetune.py \
    --config_file_path "openvla/openvla-7b" \
    --dataset_name "libero_spatial_no_noops" \
    --batch_size 8 \
    --learning_rate 5e-4 \
    --max_steps 20000 \
    --use_l1_regression True \
    --use_craft False \                            # å…³é—­ CRaFT
    --wandb_project "vla-baseline"
```

**Baseline ç‰¹ç‚¹**ï¼š
- ä»…ä¼˜åŒ–åŠ¨ä½œé¢„æµ‹æŸå¤± L_act
- æ— è¡¨å¾ä¿ç•™çº¦æŸ
- å®¹æ˜“å‡ºç°è¡¨å¾åå¡Œï¼ˆå°¤å…¶åœ¨ Few-Shot åœºæ™¯ï¼‰

### 2.2 CRaFT è®­ç»ƒï¼ˆçº¦æŸä¼˜åŒ–å¾®è°ƒï¼‰

```bash
python vla-scripts/finetune.py \
    --config_file_path "openvla/openvla-7b" \
    --dataset_name "libero_spatial_no_noops" \
    --batch_size 8 \
    --learning_rate 5e-4 \
    --max_steps 20000 \
    --use_l1_regression True \
    --use_craft True \                             # å¯ç”¨ CRaFT
    --craft_retention_budget 0.1 \                 # è¡¨å¾æ¼‚ç§»é¢„ç®— Îµ
    --craft_dual_lr 0.01 \                         # å¯¹å¶å­¦ä¹ ç‡ Î·_Î»
    --craft_enable_projection True \               # å¯ç”¨æ¢¯åº¦æŠ•å½±
    --craft_enable_dual True \                     # å¯ç”¨è‡ªé€‚åº” Î»
    --craft_anchor_type "concat" \                 # é”šç‚¹ç‰¹å¾ç±»å‹
    --wandb_project "vla-craft"
```

**CRaFT æ ¸å¿ƒå‚æ•°è¯¦è§£**ï¼š

| å‚æ•° | é»˜è®¤å€¼ | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|--------|----------|------|
| `craft_retention_budget` | 0.1 | 0.05-0.2 | å…è®¸çš„æœ€å¤§è¡¨å¾æ¼‚ç§»é‡ Îµï¼ˆè¶Šå°è¶Šä¿å®ˆï¼‰ |
| `craft_dual_lr` | 0.01 | 0.001-0.1 | å¯¹å¶å˜é‡å­¦ä¹ ç‡ Î·_Î»ï¼ˆæ§åˆ¶ Î» æ›´æ–°é€Ÿåº¦ï¼‰ |
| `craft_enable_projection` | True | True/False | æ˜¯å¦å¯ç”¨å†²çªæ„ŸçŸ¥æ¢¯åº¦æŠ•å½± |
| `craft_enable_dual` | True | True/False | æ˜¯å¦å¯ç”¨è‡ªé€‚åº” Î»ï¼ˆFalse åˆ™ä½¿ç”¨å›ºå®š Î»ï¼‰ |
| `craft_fixed_lambda` | 0.1 | 0.01-1.0 | å½“ `enable_dual=False` æ—¶çš„å›ºå®š Î» å€¼ |
| `craft_anchor_type` | "concat" | concat/aq_only/raw_only | é”šç‚¹ç‰¹å¾ç±»å‹ï¼ˆæ¶ˆèå®éªŒç”¨ï¼‰ |

---

## <a name="metrics-explained"></a>3. æ ¸å¿ƒæŒ‡æ ‡æ·±åº¦è§£è¯»

### 3.1 æ ‡å‡†è®­ç»ƒæŒ‡æ ‡

#### `VLA Train/Loss`ï¼ˆåŠ¨ä½œé¢„æµ‹æŸå¤±ï¼‰
- **å®šä¹‰**ï¼šL1 å›å½’æŸå¤±ï¼Œè¡¡é‡é¢„æµ‹åŠ¨ä½œä¸çœŸå®åŠ¨ä½œçš„å·®è·
- **è®¡ç®—å…¬å¼**ï¼š`L_act = ||Ï€_Î¸(o, l) - A_t||_1`
- **æœŸæœ›è¶‹åŠ¿**ï¼šæŒç»­ä¸‹é™ï¼Œæœ€ç»ˆæ”¶æ•›åˆ° 0.05-0.15
- **å¼‚å¸¸æƒ…å†µ**ï¼š
  - æŸå¤±ä¸ä¸‹é™ï¼šå­¦ä¹ ç‡è¿‡å°æˆ–æ•°æ®é—®é¢˜
  - æŸå¤±éœ‡è¡ï¼šå­¦ä¹ ç‡è¿‡å¤§æˆ–æ‰¹æ¬¡å¤§å°è¿‡å°

#### `VLA Train/Curr Action L1 Loss`ï¼ˆå½“å‰åŠ¨ä½œæŸå¤±ï¼‰
- **å®šä¹‰**ï¼šä»…è®¡ç®—å½“å‰æ—¶é—´æ­¥åŠ¨ä½œçš„ L1 æŸå¤±
- **æ„ä¹‰**ï¼šåæ˜ æ¨¡å‹å¯¹å³æ—¶åŠ¨ä½œçš„é¢„æµ‹èƒ½åŠ›
- **æœŸæœ›å€¼**ï¼š< 0.1ï¼ˆè¶Šå°è¶Šå¥½ï¼‰

#### `VLA Train/Next Actions L1 Loss`ï¼ˆæœªæ¥åŠ¨ä½œæŸå¤±ï¼‰
- **å®šä¹‰**ï¼šè®¡ç®—æœªæ¥ K æ­¥åŠ¨ä½œçš„å¹³å‡ L1 æŸå¤±
- **æ„ä¹‰**ï¼šåæ˜ æ¨¡å‹çš„é•¿æœŸè§„åˆ’èƒ½åŠ›
- **æœŸæœ›å€¼**ï¼šé€šå¸¸æ¯”å½“å‰åŠ¨ä½œæŸå¤±ç•¥é«˜ï¼ˆ0.1-0.2ï¼‰

### 3.2 CRaFT æ ¸å¿ƒæŒ‡æ ‡ï¼ˆâ­ è®ºæ–‡å…³é”®è¯æ®ï¼‰

#### `CRaFT/Retention Loss`ï¼ˆè¡¨å¾ä¿ç•™æŸå¤±ï¼‰
- **å®šä¹‰**ï¼šå½“å‰ç‰¹å¾ä¸é”šç‚¹ç‰¹å¾çš„å‡æ–¹è¯¯å·®
- **è®¡ç®—å…¬å¼**ï¼š`L_ret = ||f_Î¸(o, l) - fÌƒ(o, l)||Â²`
- **ç‰©ç†æ„ä¹‰**ï¼šè¡¡é‡æ¨¡å‹è¡¨å¾ç›¸å¯¹äºé¢„è®­ç»ƒçŠ¶æ€çš„æ¼‚ç§»ç¨‹åº¦
- **æœŸæœ›è¶‹åŠ¿**ï¼š
  - è®­ç»ƒåˆæœŸï¼šå¿«é€Ÿä¸Šå‡ï¼ˆæ¨¡å‹å¼€å§‹é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼‰
  - è®­ç»ƒä¸­æœŸï¼šç¨³å®šåœ¨ Îµ é™„è¿‘ï¼ˆå¯¹å¶ä¼˜åŒ–ç”Ÿæ•ˆï¼‰
  - è®­ç»ƒåæœŸï¼šä¿æŒç¨³å®šæˆ–ç•¥å¾®ä¸‹é™
- **å¼‚å¸¸æƒ…å†µ**ï¼š
  - æŒç»­ä¸Šå‡è¶…è¿‡ Îµï¼šå¯¹å¶å­¦ä¹ ç‡ `craft_dual_lr` è¿‡å°
  - å§‹ç»ˆæ¥è¿‘ 0ï¼š`craft_retention_budget` è¿‡å°ï¼Œæ¨¡å‹è¢«è¿‡åº¦çº¦æŸ

#### `CRaFT/Lambda`ï¼ˆæ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼‰
- **å®šä¹‰**ï¼šå¯¹å¶å˜é‡ Î»ï¼Œæ§åˆ¶è¡¨å¾ä¿ç•™æŸå¤±çš„æƒé‡
- **æ›´æ–°è§„åˆ™**ï¼š`Î» â† max(0, Î» + Î·_Î» Ã— (L_ret - Îµ))`
- **ç‰©ç†æ„ä¹‰**ï¼š
  - Î» å¢å¤§ï¼šæ¨¡å‹è¡¨å¾æ¼‚ç§»è¶…å‡ºé¢„ç®—ï¼Œéœ€è¦åŠ å¼ºçº¦æŸ
  - Î» å‡å°ï¼ˆè¶‹å‘ 0ï¼‰ï¼šè¡¨å¾æ¼‚ç§»åœ¨é¢„ç®—å†…ï¼Œå¯ä»¥æ”¾æ¾çº¦æŸ
- **æœŸæœ›è¶‹åŠ¿**ï¼š
  - è®­ç»ƒåˆæœŸï¼šä» 0 å¿«é€Ÿä¸Šå‡
  - è®­ç»ƒä¸­æœŸï¼šåœ¨æŸä¸ªç¨³å®šå€¼é™„è¿‘éœ‡è¡
  - è®­ç»ƒåæœŸï¼šä¿æŒç¨³å®š
- **å…¸å‹å€¼èŒƒå›´**ï¼š0.1-1.0ï¼ˆå–å†³äº `craft_dual_lr` å’Œ `craft_retention_budget`ï¼‰

#### `CRaFT/Conflict Ratio`ï¼ˆâ­ æ¢¯åº¦å†²çªç‡ï¼Œè®ºæ–‡æ ¸å¿ƒæŒ‡æ ‡ï¼‰
- **å®šä¹‰**ï¼šåœ¨æ‰€æœ‰å‚æ•°ä¸­ï¼Œå‡ºç°"åŠ¨ä½œæ¢¯åº¦"ä¸"è¡¨å¾æ¢¯åº¦"å‡ ä½•å†²çªçš„å‚æ•°æ¯”ä¾‹
- **è®¡ç®—å…¬å¼**ï¼š`Conflict Ratio = (å†²çªå‚æ•°æ•°é‡) / (æ€»å‚æ•°æ•°é‡)`
- **å†²çªåˆ¤å®š**ï¼šå½“ `<g_act, g_ret> < 0` æ—¶ï¼Œè®¤ä¸ºå‘ç”Ÿå†²çª
- **ç‰©ç†æ„ä¹‰**ï¼š
  - **é«˜å†²çªç‡ (>30%)**ï¼šæ¨¡å‹æ­£åœ¨ç»å†ä¸¥é‡çš„è¡¨å¾åå¡Œï¼ŒåŠ¨ä½œä¼˜åŒ–ä¸è¡¨å¾ä¿ç•™å­˜åœ¨å¤§é‡å†²çª
  - **ä½å†²çªç‡ (<10%)**ï¼šä¸¤ä¸ªä¼˜åŒ–ç›®æ ‡åŸºæœ¬ä¸€è‡´ï¼Œè¡¨å¾ç¨³å®š
  - **ä¸­ç­‰å†²çªç‡ (10%-30%)**ï¼šæ­£å¸¸çš„ä¼˜åŒ–è¿‡ç¨‹ï¼ŒCRaFT çš„æ¢¯åº¦æŠ•å½±æœºåˆ¶æ­£åœ¨åŒ–è§£å†²çª
- **æœŸæœ›è¶‹åŠ¿**ï¼š
  - **Baselineï¼ˆæ—  CRaFTï¼‰**ï¼šå†²çªç‡æŒç»­é«˜ä¼ï¼ˆ30%-50%ï¼‰ï¼Œè¯´æ˜è¡¨å¾åå¡Œä¸¥é‡
  - **CRaFTï¼ˆæœ‰æ¢¯åº¦æŠ•å½±ï¼‰**ï¼šå†²çªç‡é€æ¸ä¸‹é™å¹¶ç¨³å®šåœ¨è¾ƒä½æ°´å¹³ï¼ˆ5%-15%ï¼‰
- **è®ºæ–‡ä»·å€¼**ï¼š
  - è¿™æ˜¯è¯æ˜"è¡¨å¾åå¡Œ"ç°è±¡å­˜åœ¨çš„ç›´æ¥è¯æ®
  - CRaFT çš„æ¢¯åº¦æŠ•å½±èƒ½æœ‰æ•ˆåŒ–è§£å†²çªï¼Œæ˜¯è®ºæ–‡çš„æ ¸å¿ƒå–ç‚¹

### 3.3 å·¥ç¨‹å¥å£®æ€§æŒ‡æ ‡ï¼ˆPhase 7.5 æ–°å¢ï¼‰

#### `VLA Train/Gradient Norm`ï¼ˆæ¢¯åº¦èŒƒæ•°ï¼‰
- **å®šä¹‰**ï¼šæ‰€æœ‰å¯è®­ç»ƒå‚æ•°æ¢¯åº¦çš„ L2 èŒƒæ•°
- **è®¡ç®—å…¬å¼**ï¼š`||âˆ‡Î¸||_2 = sqrt(Î£ ||âˆ‡Î¸_i||Â²)`
- **ç‰©ç†æ„ä¹‰**ï¼šè¡¡é‡æ¢¯åº¦çš„æ•´ä½“å¤§å°ï¼Œåæ˜ è®­ç»ƒç¨³å®šæ€§
- **æœŸæœ›è¶‹åŠ¿**ï¼š
  - è®­ç»ƒåˆæœŸï¼šè¾ƒå¤§ï¼ˆ1.0-10.0ï¼‰
  - è®­ç»ƒä¸­æœŸï¼šé€æ¸ä¸‹é™
  - è®­ç»ƒåæœŸï¼šç¨³å®šåœ¨è¾ƒå°å€¼ï¼ˆ0.1-1.0ï¼‰
- **å¼‚å¸¸æƒ…å†µ**ï¼š
  - æ¢¯åº¦çˆ†ç‚¸ï¼šæ¢¯åº¦èŒƒæ•°çªç„¶é£™å‡åˆ° >100ï¼Œéœ€è¦é™ä½å­¦ä¹ ç‡æˆ–å¯ç”¨æ¢¯åº¦è£å‰ª
  - æ¢¯åº¦æ¶ˆå¤±ï¼šæ¢¯åº¦èŒƒæ•° <0.001ï¼Œæ¨¡å‹åœæ­¢å­¦ä¹ 

#### `VLA Train/Learning Rate`ï¼ˆå­¦ä¹ ç‡ï¼‰
- **å®šä¹‰**ï¼šå½“å‰ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
- **è°ƒåº¦ç­–ç•¥**ï¼š
  - Warmup é˜¶æ®µï¼šä» 10% çº¿æ€§å¢é•¿åˆ° 100%
  - ç¨³å®šé˜¶æ®µï¼šä¿æŒæ’å®š
  - Decay é˜¶æ®µï¼šåœ¨ `num_steps_before_decay` åè¡°å‡ 10 å€
- **æœŸæœ›è¶‹åŠ¿**ï¼š
  - 0-1000 æ­¥ï¼šä» 5e-5 å¢é•¿åˆ° 5e-4ï¼ˆWarmupï¼‰
  - 1000-20000 æ­¥ï¼šä¿æŒ 5e-4
  - 20000+ æ­¥ï¼šè¡°å‡åˆ° 5e-5

---

## <a name="experiment-scripts"></a>4. è‡ªåŠ¨åŒ–å®éªŒè„šæœ¬ä½¿ç”¨

æˆ‘ä»¬æä¾›äº†å®Œæ•´çš„è‡ªåŠ¨åŒ–è„šæœ¬ï¼Œå¯ä¸€é”®å¤ç°è®ºæ–‡ä¸­çš„æ‰€æœ‰å®éªŒã€‚

### 4.1 ä¸»å®éªŒï¼šTable 1ï¼ˆLIBERO å››ä¸ª Suiteï¼‰

**è„šæœ¬ä½ç½®**ï¼š`craft_experiments/01_main_results/run_table1_experiments.sh`

**è¿è¡Œæ–¹å¼**ï¼š
```bash
cd craft_experiments/01_main_results
bash run_table1_experiments.sh
```

**å®éªŒå†…å®¹**ï¼š
- åœ¨ LIBERO å››ä¸ª Suiteï¼ˆSpatial, Object, Goal, Longï¼‰ä¸Šåˆ†åˆ«è¿è¡Œï¼š
  - Baseline è®­ç»ƒï¼ˆæ—  CRaFTï¼‰
  - CRaFT è®­ç»ƒï¼ˆå®Œæ•´é…ç½®ï¼‰
- æ¯ä¸ªå®éªŒä½¿ç”¨ 3 ä¸ªéšæœºç§å­ï¼ˆ42, 123, 456ï¼‰
- è‡ªåŠ¨ä¿å­˜ WandB æ—¥å¿—å’Œæ¨¡å‹ Checkpoint

**é¢„æœŸè¿è¡Œæ—¶é—´**ï¼š
- å•ä¸ªå®éªŒï¼šçº¦ 4-8 å°æ—¶ï¼ˆå–å†³äº GPU æ€§èƒ½ï¼‰
- æ€»è®¡ï¼šçº¦ 3-5 å¤©ï¼ˆ8 ä¸ªé…ç½® Ã— 3 ä¸ªç§å­ Ã— 4-8 å°æ—¶ï¼‰

**è¾“å‡ºç»“æœ**ï¼š
- WandB æ—¥å¿—ï¼š`wandb_project/table1-{suite}-{method}-seed{seed}`
- Checkpointï¼š`runs/table1-{suite}-{method}-seed{seed}/`

### 4.2 æå°‘æ ·æœ¬å®éªŒï¼ˆFew-Shotï¼‰

**è„šæœ¬ä½ç½®**ï¼š`craft_experiments/02_stability_efficiency/run_fewshot_experiments.sh`

**è¿è¡Œæ–¹å¼**ï¼š
```bash
cd craft_experiments/02_stability_efficiency
bash run_fewshot_experiments.sh
```

**å®éªŒå†…å®¹**ï¼š
- åœ¨ LIBERO-Spatial ä¸Šæµ‹è¯•ï¼š
  - 5-Shotï¼ˆæ¯ä¸ªä»»åŠ¡ 5 æ¡è½¨è¿¹ï¼‰
  - 10-Shotï¼ˆæ¯ä¸ªä»»åŠ¡ 10 æ¡è½¨è¿¹ï¼‰
- å¯¹æ¯” Baseline vs CRaFT
- æ¯ä¸ªå®éªŒä½¿ç”¨ 3 ä¸ªéšæœºç§å­

**æ ¸å¿ƒå‚æ•°**ï¼š
```bash
--n_shot_episodes 5   # æˆ– 10
```

### 4.3 æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰

**è„šæœ¬ä½ç½®**ï¼š`craft_experiments/03_ablations/run_ablation_experiments.sh`

**è¿è¡Œæ–¹å¼**ï¼š
```bash
cd craft_experiments/03_ablations
bash run_ablation_experiments.sh
```

**å®éªŒå†…å®¹**ï¼š

| å®éªŒåç§° | é…ç½® | ç›®çš„ |
|---------|------|------|
| CRaFT (Full) | `projection=True, dual=True` | å®Œæ•´ CRaFT |
| w/o Projection | `projection=False, dual=True` | éªŒè¯æ¢¯åº¦æŠ•å½±çš„ä½œç”¨ |
| w/o Dual | `projection=True, dual=False, fixed_lambda=0.1` | éªŒè¯è‡ªé€‚åº” Î» çš„ä½œç”¨ |
| AQ Only | `anchor_type=aq_only` | ä»…ä½¿ç”¨åŠ¨ä½œæŸ¥è¯¢ç‰¹å¾ |
| Raw Only | `anchor_type=raw_only` | ä»…ä½¿ç”¨åŸå§‹æ½œåœ¨ç‰¹å¾ |

---

## <a name="monitoring-debugging"></a>5. è®­ç»ƒç›‘æ§ä¸è°ƒè¯•

### 5.1 å®æ—¶ç›‘æ§ WandB æ—¥å¿—

è®­ç»ƒå¯åŠ¨åï¼Œè®¿é—® WandB é¡¹ç›®é¡µé¢æŸ¥çœ‹å®æ—¶æŒ‡æ ‡ï¼š

```
https://wandb.ai/{your-entity}/{your-project}
```

**å…³é”®å›¾è¡¨**ï¼š
1. **Loss æ›²çº¿**ï¼š`VLA Train/Loss` åº”æŒç»­ä¸‹é™
2. **CRaFT æŒ‡æ ‡**ï¼š
   - `CRaFT/Retention Loss` åº”ç¨³å®šåœ¨ Îµ é™„è¿‘
   - `CRaFT/Lambda` åº”åœ¨è®­ç»ƒä¸­æœŸç¨³å®š
   - `CRaFT/Conflict Ratio` åº”é€æ¸ä¸‹é™
3. **æ¢¯åº¦èŒƒæ•°**ï¼š`VLA Train/Gradient Norm` åº”ä¿æŒç¨³å®š

### 5.2 ç»ˆç«¯æ—¥å¿—è§£è¯»

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç»ˆç«¯ä¼šæ˜¾ç¤ºå®æ—¶è¿›åº¦æ¡ï¼š

```
Loss: 0.1234 | Î»: 0.456 | Conflict: 12.34% | GradNorm: 1.23 | LR: 5.00e-04
```

**å­—æ®µè¯´æ˜**ï¼š
- `Loss`ï¼šå½“å‰æ‰¹æ¬¡çš„åŠ¨ä½œæŸå¤±
- `Î»`ï¼šå½“å‰çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­
- `Conflict`ï¼šå½“å‰æ‰¹æ¬¡çš„æ¢¯åº¦å†²çªç‡
- `GradNorm`ï¼šå½“å‰æ‰¹æ¬¡çš„æ¢¯åº¦èŒƒæ•°
- `LR`ï¼šå½“å‰å­¦ä¹ ç‡

### 5.3 Checkpoint ç®¡ç†

**ä¿å­˜ç­–ç•¥**ï¼š
- é»˜è®¤æ¯ 5000 æ­¥ä¿å­˜ä¸€æ¬¡ Checkpoint
- å¯é€šè¿‡ `--save_freq` è°ƒæ•´ä¿å­˜é¢‘ç‡
- å¯é€šè¿‡ `--save_latest_checkpoint_only True` ä»…ä¿å­˜æœ€æ–° Checkpointï¼ˆèŠ‚çœç£ç›˜ç©ºé—´ï¼‰

**Checkpoint å†…å®¹**ï¼š
```
runs/experiment-name--10000_chkpt/
â”œâ”€â”€ config.json                    # æ¨¡å‹é…ç½®
â”œâ”€â”€ processor_config.json          # å¤„ç†å™¨é…ç½®
â”œâ”€â”€ lora_adapter/                  # LoRA æƒé‡ï¼ˆå¦‚æœä½¿ç”¨ LoRAï¼‰
â”œâ”€â”€ action_head--10000_checkpoint.pt        # åŠ¨ä½œå¤´æƒé‡
â”œâ”€â”€ dataset_statistics.json        # æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
â””â”€â”€ training_state.pt              # è®­ç»ƒçŠ¶æ€ï¼ˆOptimizer + Schedulerï¼‰
```

### 5.4 æ–­ç‚¹ç»­è®­

```bash
python vla-scripts/finetune.py \
    --config_file_path "runs/experiment-name--10000_chkpt" \
    --resume True \
    --resume_step 10000 \
    --max_steps 20000 \
    --use_craft True
```

**è¯´æ˜**ï¼š
- `training_state.pt` åŒ…å« Optimizer å’Œ Scheduler çš„å®Œæ•´çŠ¶æ€
- ç»­è®­åä¼šä» Step 10001 å¼€å§‹ï¼Œå­¦ä¹ ç‡å’ŒåŠ¨é‡ç­‰çŠ¶æ€å®Œå…¨æ¢å¤

---

## <a name="troubleshooting"></a>6. å¸¸è§æŠ¥é”™ä¸æ’æŸ¥

### é—®é¢˜ 1ï¼šCUDA Out of Memory (OOM)

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**åŸå› **ï¼šGPU æ˜¾å­˜ä¸è¶³ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ–¹æ¡ˆ 1ï¼šå‡å° Batch Size
--batch_size 4  # ä» 8 é™ä½åˆ° 4

# æ–¹æ¡ˆ 2ï¼šå¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆä¿æŒæ€»æ‰¹æ¬¡å¤§å°ä¸å˜ï¼‰
--batch_size 4 \
--grad_accumulation_steps 2  # æ€»æ‰¹æ¬¡ = 4 Ã— 2 = 8

# æ–¹æ¡ˆ 3ï¼šå‡å° Shuffle Buffer Size
--shuffle_buffer_size 10000  # ä» 100000 é™ä½åˆ° 10000
```

### é—®é¢˜ 2ï¼šLoss ä¸ä¸‹é™æˆ–éœ‡è¡

**å¯èƒ½åŸå› **ï¼š
1. å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°
2. æ‰¹æ¬¡å¤§å°è¿‡å°
3. æ•°æ®é—®é¢˜

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. é™ä½å­¦ä¹ ç‡
--learning_rate 1e-4  # ä» 5e-4 é™ä½åˆ° 1e-4

# 2. å¢å¤§æ‰¹æ¬¡å¤§å°
--batch_size 16 \
--grad_accumulation_steps 2  # æ€»æ‰¹æ¬¡ = 32

# 3. æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½
python -c "
from prismatic.vla.datasets import RLDSDataset
dataset = RLDSDataset(...)
print(f'Dataset size: {len(dataset)}')
"
```

### é—®é¢˜ 3ï¼šCRaFT Retention Loss æŒç»­ä¸Šå‡

**é”™è¯¯ç°è±¡**ï¼š`CRaFT/Retention Loss` è¿œè¶… `craft_retention_budget`ï¼Œä¸” `Lambda` æŒç»­å¢å¤§ã€‚

**åŸå› **ï¼šå¯¹å¶å­¦ä¹ ç‡ `craft_dual_lr` è¿‡å°ï¼ŒÎ» æ›´æ–°é€Ÿåº¦ä¸å¤Ÿå¿«ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å¢å¤§å¯¹å¶å­¦ä¹ ç‡
--craft_dual_lr 0.05  # ä» 0.01 å¢å¤§åˆ° 0.05
```

### é—®é¢˜ 4ï¼šæ¢¯åº¦çˆ†ç‚¸

**é”™è¯¯ä¿¡æ¯**ï¼š
```
Loss: nan | GradNorm: inf
```

**åŸå› **ï¼šæ¢¯åº¦èŒƒæ•°è¿‡å¤§ï¼Œå¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ–¹æ¡ˆ 1ï¼šé™ä½å­¦ä¹ ç‡
--learning_rate 1e-4

# æ–¹æ¡ˆ 2ï¼šå¯ç”¨æ¢¯åº¦è£å‰ªï¼ˆéœ€ä¿®æ”¹ä»£ç ï¼‰
# åœ¨ finetune.py ä¸­æ·»åŠ ï¼š
torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
```

### é—®é¢˜ 5ï¼šDDP å¤šå¡è®­ç»ƒæŠ¥é”™

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: Expected to have finished reduction in the prior iteration
```

**åŸå› **ï¼šDDP æ¢¯åº¦åŒæ­¥é—®é¢˜ï¼Œé€šå¸¸ç”± `find_unused_parameters` å¼•èµ·ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨ finetune.py ä¸­æ£€æŸ¥ DDP é…ç½®
vla = DDP(vla, device_ids=[device_id], find_unused_parameters=True)
```

### é—®é¢˜ 6ï¼šWandB ç¦»çº¿æ¨¡å¼

**ç°è±¡**ï¼šWandB æ—¥å¿—æœªä¸Šä¼ åˆ°äº‘ç«¯ã€‚

**åŸå› **ï¼šè®­ç»ƒè„šæœ¬ä¸­è®¾ç½®äº† `mode="offline"`ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä¿®æ”¹ finetune.py ä¸­çš„ WandB åˆå§‹åŒ–
wandb.init(project=cfg.wandb_project, name=f"ft+{run_id}", mode="online")  # æ”¹ä¸º online
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **[æ•°æ®é›†å‡†å¤‡æŒ‡å—](DATASETS.md)**ï¼šLIBERO æ•°æ®é›†ä¸‹è½½å’Œé…ç½®
- **[é¡¹ç›®ç»“æ„è¯¦è§£](craft/PROJECT_STRUCTURE.md)**ï¼šä»£ç åº“æ¶æ„æ·±åº¦è§£æ
- **[ä¸» README](../README.md)**ï¼šé¡¹ç›®æ¦‚è§ˆå’Œå¿«é€Ÿå¼€å§‹

---

## ğŸ¤ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°è®­ç»ƒç›¸å…³é—®é¢˜ï¼š
1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„"å¸¸è§æŠ¥é”™ä¸æ’æŸ¥"ç« èŠ‚
2. æ£€æŸ¥ WandB æ—¥å¿—ä¸­çš„å¼‚å¸¸æŒ‡æ ‡
3. æäº¤ GitHub Issue å¹¶é™„ä¸Šï¼š
   - å®Œæ•´çš„è®­ç»ƒå‘½ä»¤
   - é”™è¯¯æ—¥å¿—
   - WandB æ—¥å¿—é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰

---

**æœ€åæ›´æ–°**ï¼š2024-02-27 | **ç»´æŠ¤è€…**ï¼šVLA-Adapter Team

